---
title: "Introduction to Statistical Analysis"
author: "Mark Dunning and Sarah Vowler"
date: '`r format(Sys.time(), "Last modified: %d %b %Y")`'
output:  BiocStyle::pdf_document
toc: TRUE
---

# Introduction

```{r eval=TRUE, echo=F, results="asis"}
BiocStyle::markdown()
library("knitr")
opts_chunk$set(tidy=FALSE,dev="png",fig.show="as.is",
               fig.width=10,fig.height=4,
               message=FALSE,eval=TRUE,warning=FALSE,echo=TRUE)
```

![fisher](images/fisher.jpg)

> *To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.* - R.A. Fisher, 1938

The goals of statistical methods could be summarised as follows
+ drawing conclusions about a population by analysing data on just a sample
+ evaluating the uncertainty in these conclusions; and
+ designing the sampling approach so that valid and accurate conclusions can be made from the data collected

# Thinking about your analysis

Statistical tests are used to help us draw conclusions about one or more populations. As populations can be very large, we usually take a sample to collect our data on and perform statistical analyses on this sample data. It is never too early in the research process to start thinking about the statistical methods you will use.

![flow](images/flowchart.jpg)

At the design stage of a study, it is invaluable to think about the data that your experiment will generate, how that data might be analysed and what size of effects may be detected. 

We want to generalise our findings from a sample of observations. Most statistical methods rely on the assumption that each observation is sampled independently of the others. In this context, independence means that one observation has no influence over other observations in the sample, or that one observation gives us no information about other observations within the sample. For example:

+ Suppose you are interested in a measurement taken from each of 20 individuals}. If there is no reason why the measurement for subject 1 should be more related to another measurement in the set of 20 measurements than any other, e.g. no siblings amongst the 20 individuals, only a single measurement from each individual, then in this situation we can say that the 20 measurements are independent of each other.
+ Suppose you are repeating an experiment six times, each time in triplicate}. The 18 measurements are not independent as observations within an experiment may be more similar than observations from separate experiments. The experimental conditions (e.g. temperatures, reagent preparations, extraction date, etc.) may differ between experiments. We could get six independent mean values by calculating the mean of each triplicate.
+ Suppose you are measuring blood pressure before and after a treatment for 30 patients}. You will not have 60 independent measurements as we have two sets of measurements per patient. Measurements within a patient may be more similar than between patients. In addition, the treatment may affect the blood pressure measurements taken afterwards. Therefore, measurements before and after treatment are not necessarily comparable. However, for each patient, we can calculate the difference between the measurements before and after treatment. The 30 differences are independent and we might test whether these differences are significant using a t test (more later!).
+ Suppose you are measuring protein expression in a cell sample which may be one of five cell-types and collected from one of three mice}. As you might be able to tell by now, these 15 samples would not be independent. The protein expression may depend on the cell-type and which of the three mice the sample was collected from. This example is tricky(!) and how you might handle these data will depend on your research question. Advance planning will certainly help.}

The type of data you will get will determine which analyses will be most suitable. Data take two main forms – **categorical** or **numerical**.

**Categorical** observations are allocations of individuals to one of two or more classes or categories. These categories may be **nominal** or **ordinal** (some natural ordering of the categories). 

Examples of **nominal** data are: 
+ Sex - Male/female; 
+ Disease status 	– diseased/non-diseased; 
+ Treatment status – treated/non-treated.

Examples of **ordinal** data are: 
+ Smoking – non-smoker/ex-	smoker/light smoker/heavy smoker; 
+ Stage of breast cancer – 	0/1/2/3/4; 
+ socioeconomic status – low/middle/high.

**Numerical** observations may be **discrete** or **continuous**. Discrete numerical variables are often counts of events whereas continuous numerical variables are mostly measurements of some form.

Examples of **discrete** data are: 
+ Number of children; 
+ Sequence tag 	counts (from ChIP-seq); Number of relapses.

Examples of **continuous** data are: 
+ Body Mass Index (BMI); 	
+ Temperature; Blood pressure; Tumour width

# Software for Statistical Analysis

# Statistical Analysis

The statistical approach used is dependent on the data type. In this document we will describe **t-tests** (including one-sample, independent two-sample and paired two-sample), which can be used when we have one or two groups of **continuous numerical** data, and **contingency tables** (chi-squared test and Fisher's exact test), which can be used when we have two **categorical** variables (which may be ordinal).

## Exploratory Analysis

Before conducting the formal analysis of our data, it is always a good idea to run some exploratory checks of the data: 
+ a) To check that the data has been read in or entered correctly; 
+ b) To identify any outlying values and if there is reason to question their validity, exclude them or investigate them further; 
+ c) To see the distribution of the observations and whether the planned analyses are appropriate.

## Summary Statistics

Summary statistics give you an initial impression of the data – the most common measures of the location and spread of the data are, respectively, the **mean** and **standard deviation**. The **mean** is the sum of all observations divided by the number of observations.

E.g. No. of facebook friends for 7 colleagues: $311, 345, 270, 310, 243, 5300, 11$

The mean is given by:

$$X = \frac{311+345+270+310+243+5300+11}{7} = 970 $$

The **standard deviation** is the square root of the variance of our observations. The variance is the sum of squared differences between each observation and the mean divided by the number of observations.

$$s.d = \sqrt{\frac{(311-970)^2 + (345 - 970)^2 + \dots (11-970)^2}{7}}=1913$$

**When the data are *skewed*** by several extreme, but valid, observations (see example histogram below), the **median** and **interquartile range** may be more meaningful summary measures than the mean and standard deviation. With the extreme observation 5300 in our data-set, the median and interquartile range could provide a more suitable summary of our data. The *median* is the middle observation when the data is ranked in order. When there is an even number of observations, the median is the mean of the two central values.

Therefore, the median of the above data is *310*:

11,243,270, *310*, 311,345,5300

The *interquartile range* is the difference between the upper and lower quartiles (or 75th and 25th centiles). The quartiles are identified in a similar fashion to the median - the middle observations of the lower and upper halves of the data, as *243* and *345*:

11,*243*,270 310, 311,*345*,5300. 

Therefore, the interquartile range is $345-243=\textbf{102}$.


## Outliers, or missing data?

Summary statistics and exploratory plots can help us identify missing and strange values – both of which can have considerable influence on our analyses. Missing values can bias our conclusions unless we can make the assumption that these values are missing completely at random (i.e. independent of observed and unobserved variables). Outlying values should only be excluded if there is reason to question their validity. Therefore, complete and accurate data collection is important.

Let’s suppose that upon further investigation, the **5300** Facebook friends observed – was erroneous and should have actually been input as *530*. We recalculate the mean and standard deviation as: *289* and *154* respectively.

## Standard error of the mean vs. Standard Deviation

The standard deviation is often confused with the standard error of the mean. 

![flow](images/hists.png)

The *standard deviation* quantifies the spread or variability of the observations. In Figure \ref{histograms} can see that the standard deviation of the observations in *Group 1* is **greater** than the standard deviation of the observations in *Group 2*. The *standard error of the mean* quantifies how precisely we know the true mean. The standard error of the mean is the standard deviation divided by the square root of the number of observations. In our example data, the standard error of the mean (s.e.m.) is given by: 

$$ se = \frac{sd}{\sqrt{n}} = \frac{154}{\sqrt{7}} = 58 $$

We can use the *standard error of the mean* to calculate a *confidence interval* for the mean. The *confidence interval* indicates the uncertainty about the estimate. 

The normal distribution plays an important part in confidence interval estimation. Essentially, the frequency distribution of the sample mean is normal, irrespective of the shape of the frequency distribution of the population from which the sample comes. The approximation is better if the population itself is reasonably normal but even if not it gets closer as the sample size increases.

```{r echo=FALSE,fig.show='asis',fig.height=6,fig.width=10,warning=FALSE,message=FALSE}
library(RColorBrewer)
library(ggplot2)
library(gridExtra)
library(grid)
x     = rnorm(10000, 0, 1)
sd <- sd(x)
me <- mean(x)

dat <- data.frame(x=x)
rects <- data.frame(xstart = c(me-2.58,me-1.96,me-1), xend = c(me+2.58,me+1.96,me+1),col=letters[1:3])



normdist <- ggplot() +   geom_histogram(data = dat, aes(x))+  geom_rect(data = rects, aes(xmin = xstart, xmax = xend, ymin = -Inf, ymax = Inf,fill=col),alpha=0.4) + ylab("") + xlab("") + geom_segment(aes(x = me-1, y = 999, xend = me +1, yend=999),arrow=arrow(length=unit(0.5, "cm"), ends="both")) + geom_segment(aes(x = me-1.96, y = 959, xend = me +1.96, yend=959),arrow=arrow(length=unit(0.5, "cm"), ends="both"))  + 
geom_segment(aes(x = me-2.58, y = 919, xend = me +2.58, yend=919),arrow=arrow(length=unit(0.5, "cm"), ends="both")) + theme(legend.position="none") + annotate("text", x=me, y=980,label="67% of data",color="red") + annotate("text", x=me+1, y=750,label="mean + s.e",color="blue") + annotate("text", x=me-1, y=750,label="mean - s.e",color="blue") + annotate("text", x=me, y=940,label="95% of data",color="red") + annotate("text", x=me-1.96, y=550,label="mean - 1.96s.e",color="blue") + annotate("text", x=me+1.96, y=550,label="mean + 1.96 s.e",color="blue") + annotate("text", x=me, y=900,label="99% of data",color="red") + annotate("text", x=me-2.56, y=350,label="mean - 2.56s.e",color="blue") + annotate("text", x=me+2.56, y=350,label="mean + 2.56s.e",color="blue")
            
normdist
```

It is most common to calculate a 95\% confidence interval as:

$$(mean - 1.96 s.e, mean+1.96s.e)$$

If we were to take many random samples, of equal size, from our population of interest, then the mean estimates of all these samples would be normally distributed. Amongst the 95% confidence intervals of those means, we would expect 95% to contain the true population mean. Hence, for our example above (number of Facebook friends of seven colleagues) the 95% confidence interval for mean number of Facebook friends is:

Mean 289, 95$\%$ CI ( $289 - (1.96 \times 58),  289 + (1.96 \times 58)$)\\
Mean 289, 95$\%$ CI ( 175,  402)

The confidence interval is quite wide which indicates that we are quite uncertain of the true mean number of Facebook friends of \textbf{all} colleagues.

# Graphs

One of the best ways of displaying data is by using a graph. Graphs can make both simple and complex data easier to understand by making it easier to spot trends and patterns. We can use plots to view the distribution of our data (minimum, maximum, mid-point, spread etc) and to ensure that the values in our dataset seem realistic. Many statistical tests rely on the assumption that the data are normally distributed, which can be assessed using histograms or box plots (more later).

## Scatter Plots
A scatterplot is an excellent way of displaying the relationship between two continuous variables. For example, a scatterplot can be used to show the relationship between age and height. If there is a fairly clear response variable (height in this case) it should be put on the vertical axis with the explanatory variable on the horizontal axis. 

```{r echo=FALSE,fig.show='asis',fig.height=5,fig.width=10,warning=FALSE,message=FALSE}
x <- rnorm(50,10,2)
y <- jitter(15*x + 2,amount=10)
plot(x,y,pch=16,xlab="Age",ylab="Height")
```

Scatter plots are very useful because they show every point in your dataset. You may be able to spot trends in the data and identify any unusual observations (outliers). 


## Boxplots 

A box plot is an excellent way of displaying continuous data when you are interested in the spread of your data. The box of the box plot corresponds to the lower and upper quartiles of the respective observations and the bar within the box, the median. The whiskers of the box plot correspond to the distance between the lower/upper quartile and the *smaller* of: the smaller/largest measurement **OR** 1.5 times the interquartile range. 

A disadvantage of the box plot is that you don't see the exact data points. However, box plots are very useful in large datasets where plotting all of the data may give an unclear picture of the shape of your data.

```{r echo=FALSE,fig.show='asis',fig.height=5,fig.width=10,warning=FALSE,message=FALSE}
ko <- rnorm(50,5,2)
wt <-rnorm(55,8,3)
boxplot(wt,ko,names=c("wt","ko"))
```

# Hypothesis testing - basic setup

There are four key steps in hypothesis testing:

1. Formulate a *null hypothesis*, $H_0$. This is the working hypothesis that we wish to disprove.
2. Under the assumption that the *null hypothesis* is true, calculate a *test statistic* from the data.
3. Determine whether the *test statistic* is more extreme than we would expect under the *null hypothesis*, i.e. look at the *p-value*.
4. Reject or do not reject the *null hypothesis*.

As the name suggests, the null hypothesis typically corresponds to a null effect. 

For example, there is *no difference* in the measurements in group 1 compared with group 2. A small p-value indicates that the probability of observing such a test statistic as small under the assumption that the null hypothesis is true. If the p-value is below a pre-specified *significance level*, then this is a *significant result* and, we would conclude, there is evidence to reject the null hypothesis.

The *significance level* is most commonly set at 5% and may also be thought of as the *false positive rate*. That is, there is a 5% chance that the null hypothesis is true for data-sets with test statistics corresponding to p-values of less than 0.05 – we may wrongly reject the null hypothesis when the null hypothesis is true (false positive).

```{r echo=FALSE,fig.show='asis',fig.height=5,fig.width=10,warning=FALSE,message=FALSE}
x     = rnorm(10000, 0, 1)
sd <- sd(x)
me <- mean(x)

dat <- data.frame(x=x)
rects <- data.frame(xstart = c(-1.96,1.96), xend = c(-Inf,Inf),col=c("A","A"))

normdist <- ggplot() +   geom_histogram(data = dat, aes(x))+  geom_rect(data = rects, aes(xmin = xstart, xmax = xend, ymin = -Inf, ymax = Inf),alpha=0.4,fill="yellow") + ylab("") + xlab("") + theme(legend.position="none")
            
normdist
```

Equally, we may make *false negative* conclusions from statistical tests. In other words, we may not reject the null hypothesis when the null hypothesis is, in fact, not true. When referring to the false negative rate, statisticians usually refer to *power*, which is 1-false negative rate. 

The *power* of a statistical test will depend on:


+ The *significance level* - a 5% test of significance will have a greater chance of rejecting the null than a 1% test because the strength of evidence required for rejection is less.
+ The *sample size* – the larger the sample size, the more accurate our estimates (e.g. of the mean) which means we can differentiate between the null and alternative hypotheses more clearly.
+ The *size of the difference or effect* we wish to detect – bigger differences (i.e. alternative hypotheses) are easier to detect than smaller differences.
+ The *variability*, or standard deviation, of the observations – the more variable our observations, the less accurate our estimates which means it is more difficult to differentiate between the null and alternative hypotheses.

# T-tests 

T-tests can be broken down into two main types: one-sample and two-sample. Both will be discussed below with examples of their applications. The two-sample t-test can also be broken down further into independent and paired t-tests, which will both be discussed below.

## One-Sample t-test
A one-sample t-test is the most basic t-test available and should be used if you want to test whether your population mean may be significantly different to a hypothesised mean. As it is rarely possible to measure the whole population, a sample is taken in the hope that it is representative of the wider population. 

### Example

A microarray supplier claims that their microarray failure rate is 2.1\%. Genomics would like to know whether this reflects the failure rates that they've observed over the last 12-months, so they have collected failure rate data on a monthly basis. The data is given in Table $\ref{microarrayTable}$.

```{r echo=FALSE,results="asis",message=FALSE,}
library(xtable)
data<-data.frame(Month=month.name,Failure=c(2.9,2.99,2.48,1.48,2.71,4.17,3.74,3.04,1.23,2.72,3.23,3.4))
print(xtable(data,label = "microarrayTable"),row.names=FALSE)
```

To calculate the mean for this sample we add up the 12 values of the monthly failure rate and divide this number by the number of observations in the sample, in this case 12. So the sample mean is: `r paste(data[,2],collapse=" + ")` / `r nrow(data)` = `r round(mean(data[,2]),3)`.

We can see straight away that the sample mean of `r round(mean(data[,2]),3)` is higher than our hypothesised mean of 2.1, but we cannot yet say if it is *significantly* different, that is if the difference is greater than we would expect by chance. This is where the one-sample t-test should be used.

A two-sided test is used when you want to know if the population mean is **different** to a hypothesised value. A one-sided test is used when you want to know if the population mean is either *higher* or *lower* than that hypothesised value and you can justify that observing a difference in one direction would lead to the same action/conclusion as if no difference had been observed. *A two-sided test is always favoured* unless there is a strong argument for a one-sided test. In this example, we have not specified in our hypothesis whether we think the mean monthly failure rate of the microarrays is higher or lower than 2.1\%, so we will use a two-sided t-test.

The one sample t-test is based on the formula:

$$t_{n-1} = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}$$

where;
+ $\bar{x}$ is the sample mean
+ $\mu_0$ is the hypothesised mean
+ $s$ is the sample standard deviation
+ $n$ is the sample size

The key assumptions of the one-sample t-test are;

1. The observations are independent
2. The observations are normally distributed


Before conducting the one sample t-test, we need to check the observations are normally distributed and this can be done by creating a histogram 

```{r echo=FALSE}
df<-data.frame(Month=month.name,Failure=c(2.9,2.99,2.48,1.48,2.71,4.17,3.74,3.04,1.23,2.72,3.23,3.4))
colnames(df)[2] <- "X"


  p<- ggplot(df, aes(x=X)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="black", fill="white") + ylab("") 

  p <- p + stat_function(fun=dnorm,
                           color="red",
                           arg=list(mean=mean(df$X), 
                                    sd=sd(df$X)))
  
  p
```

We want the histogram to be fairly symmetrical with a rough bell shape like that of the plot on the right hand side above. It is often difficult to assess whether data from a small sample are normally distributed, as a single observation can distort the shape of the histogram. 

Here (Figure \ref{microarrayHist}), the histogram is *fairly symmetrical with a rough bell shape* (it doesn't have to be perfect!) so the normality assumption seems reasonable. Slight deviations from normality are rarely a problem as the t-test is fairly robust. 


Assuming that the null hypothesis is true, our t-statistic comes from the t distribution with 11 degrees of freedom

$$ t_{n-1} = t_11 = \frac{\bar{x} - \mu_0}{s / \sqrt{n}} = \frac{2.84-2.10}{0.84 / \sqrt{12}} = \frac{0.74}{0.24} = 3.07 $$

```{r echo=FALSE,fig.cap="t-distribution with 11 degrees of freedom. The t-statistic 3.07 is indicated"}

mu <- 2.1

degfree <- nrow(df)-1
tstat <- t.test(df$X,mu=mu)$statistic
      
df <- data.frame(ts = rt(10000,df=degfree))
  
      
p<- ggplot(df, aes(x=ts)) + 
  geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
               binwidth=.5,
               colour="black", fill="white") +
  geom_density()

xlim <- c(min(tstat-0.2,min(df$ts)), max(tstat+0.2, max(df$ts)))

critvals <- c(qt(0.05, degfree),qt(0.95,degfree))
rect1 <- data.frame(xmin = min(critvals[1],xlim),xmax = critvals[1], ymin=-Inf,ymax=Inf)
rect2 <- data.frame(xmin = critvals[2],xmax = max(critvals[2],xlim), ymin=-Inf,ymax=Inf)
      
 p <- p + geom_rect(data=rect1,aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax),color="grey20", alpha=0.5, inherit.aes = FALSE) + geom_rect(data=rect2,aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax),color="grey20", alpha=0.5, inherit.aes = FALSE)
 p <- p + geom_vline(xintercept = tstat,lty=2,col="red") + xlim(xlim)
print(p)
```

Under the null hypothesis, that the mean monthly failure rate of the microarrays = 2.1%, we can see that the probability of observing a value of the t-statistic as extreme as 3.07 is very small. This **p-value** is

$$P(T \le 3.07 | T \ge 3.07) = 0.01 $$

```{r echo=FALSE}
df<-data.frame(Month=month.name,Failure=c(2.9,2.99,2.48,1.48,2.71,4.17,3.74,3.04,1.23,2.72,3.23,3.4))
t.test(df[,2],mu=2.1)
```

As the p-value of 0.01 is less than 0.05 (5%), there is **evidence to reject the null hypothesis** and conclude that there is evidence to suggest that the failure rate of the microarrays from this supplier is not 2.1. 

## Two-Sample t-test

A two-sample t-test should be used if you want to *compare the measurements of two populations*. There are two types of two-sample t-test: independent (unpaired) and paired (dependent).

An independent two-sample t-test is used when the two samples are *independent* of each other, e.g. comparing the mean response of two groups of patients on treatment vs. control in a clinical trial. As the name suggests, a paired two-sample t-test is used when the two samples are paired, e.g. comparing the mean blood pressure of patients before and after treatment (two measurements per patient).

### Independent two-sample t-test

$\label{MiceWeights}$

Breed A  | Breed B
------------- | -------------
1. 20.77 | 21. 15.51
2. 9.08 | 22. 12.93
3. 9.80 | 23. 11.50
4. 8.13 | 24. 16.07
5. 16.54 | 25. 15.51
6. 11.36 | 26. 17.66
7. 11.47 | 27. 11.25
8. 12.10 | 28. 13.65
9. 14.04 | 29. 14.28
10. 16.82 | 30. 13.21
11. 6.32 | 31. 10.28
12. 17.51 | 32. 12.41
13. 9.87 | 33. 9.63
14. 12.41 | 34. 14.75
15. 7.39 | 35. 9.81
16. 9.23 | 36. 13.02
17. 4.06 | 37. 12.33
18. 8.26 | 38. 11.90
19. 10.24 | 39. 8.98
20. 14.64 | 40. 11.29

*Example*: A researcher is interested in the effect of breed on weight in 4 week old male mice.  40 male mice were used, 20 of breed A and 20 of breed B. The data are shown in Table $\ref{MiceWeights}$.


So, the researcher wants to test the *null hypothesis* that the mean weight of breed A is *equal* to the mean weight of breed A in 4 week-old male mice.

+ *Mean weight of breed A = Mean weight of breed B*

Our *alternative hypothesis* is that the mean weight of breed A is *not equal* to the mean weight of breed B in 4 week-old male mice.

+ *Mean weight of breed A $\ne$ Mean weight of breed B*


To perform the independent two-sample t-test, we calculate the following t-statistic from our data:

$$t_{df} = \frac{\bar{X}_A  - \bar{X}_B}{s.e (\bar{X}_A - \bar{X}_B)}$$

where;


******

+ $\bar{X}_A$ is the mean weight of mice in breed A
+ $\bar{X}_B$ is the mean weight of mice in breed B
+ $s.e(\bar{X_A} - \bar{X}_B)$ is the standard error of the difference in mean weights
+ $df$ is the degrees of freedom and is equal the total number of mice (breed A and breed B) minus the number of independent groups (N -2)

******

### Analyis in Shiny

```{r echo=FALSE}
MiceWeight <- read.csv("data/Independent two-sample t-test.csv")
res <- t.test(Weight~Breed,data=MiceWeight)
res
```


## Interpretation of Results

The mean weight of Breed A male mice at 4 weeks old was `r res$estimate[1]`g, whilst for Breed B the mean weight was `r res$estimate[2]`g. The Welch-corrected t-statistic is given by:


$$t_{29} = \frac{\bar{X}_A - \bar{X}_B}{s.e(\bar{X}_A - \bar{X}_B)} = 1.21$$


We can see that the t-statistic we observe is consistent with the null hypothesis, that the mean weight of 4 week old male mice is the same for breeds A and B. That is, the probability of observing a t-statistic of 1.21 or more, or -1.21 or less, is quite high:

$$P(T \le -1.21 | T \ge 1.21) = 0.24$$

This is not a significant result (p$>0.05$), so there is *no evidence of a difference* in the weight of male mice at 4 weeks old between Breeds A and B. 


### Paired two-sample t-test

A  | B | Difference
------------- | ------------- |-------------
1201.33  | 1155.98 | -45.35
1029.64 | 1020.82 | -8.82
895.57 | 881.21 | -14.36
903.07 | 897.06 | -6.01
1311.57 | 1262.73 | -48.84
833.52 | 823.06 | -10.46
1007.62 | 951.01 | -56.65
1465.51 | 1450.98 | -14.53
967.82 | 978.15 | 10.33
812.72 | 778.26 | -34.46
884.08 | 823.57 | -60.51
1358.56 | 1335.78 | -22.78
1280.10 | 1293.91 | 13.81
942.38 | 925.75 | -16.63
884.33 | 891.34 | 7.01
930.09 | 892.02 | -38.07
1146.75 | 1132.80 | -13.95
881.50 | 847.78 | -33.72
1315.22 | 1337.80 | 22.58

*Example* : 20 patients with advanced cancer were studied using MRI imaging. Cellularity was measured for each individual patient by estimating water movement. We want to know whether there is a significant difference in the cellularity between two sites in the body; A and B. The data are shown in Table $\ref{cellularity}$. We want to test the *null hypothesis* that the mean cellularity at site A is equal to the mean cellularity at site B. This is like saying:

+ Mean cellularity at site A = mean cellularity at site B

Essentially, this two-sample test corresponds to a formal comparison of the *differences between each pair* of cellularities with 0 (so a one-sample t-test). We could reformulate our null hypothesis as:

+ Mean difference in cellularities at site A and site B =0 

Our *alternative hypothesis* is that the mean cellularity at site A is *not equal* to the mean cellularity at site B. This is like saying:

+ Mean cellularity at site A $\ne$ Mean cellularity at site B

To perform the paired two-sample t-test, we calculate the following t-statistic from our data:

$$ t_{n-1} = t_{19} = \frac{\bar{X}_{A - B}}{s.e.(\bar{X}_{A-B})}$$

where


+ $\bar{X}_{A-B}$ is the mean difference in cellularities between the two sites
+ $s.e(\bar{X}_{A-B})$ is the standard error of the mean difference in cellularities

The assumptions of the paired t-test coincide with those of the one-sample t-test:


+ The observed *differences* are *independent*
+ The observed *differences* are *normally distributed*
